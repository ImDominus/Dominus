{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "from time import time\n",
    "from collections import deque\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load MNIST\n",
    "#http://yann.lecun.com/exdb/mnist/\n",
    "def unpickle_MNIST(image, label):\n",
    "    #Read Image data\n",
    "    image_set = open(image, 'rb')\n",
    "    magic_number = struct.unpack(\">i\", image_set.read(4))[0]\n",
    "    number_images = struct.unpack(\">i\", image_set.read(4))[0]\n",
    "    rows = struct.unpack(\">i\", image_set.read(4))[0]\n",
    "    cols = struct.unpack(\">i\", image_set.read(4))[0]\n",
    "    image_set_data = np.reshape(np.fromstring(image_set.read(), dtype = np.uint8), (number_images, rows * cols))\n",
    "    image_set.close()\n",
    "    #Read Label data\n",
    "    label_set = open(label, 'rb')\n",
    "    magic_number = struct.unpack(\">i\", label_set.read(4))[0]\n",
    "    number_of_items = struct.unpack(\">i\", label_set.read(4))[0]\n",
    "    label_set_data = np.reshape(np.fromstring(label_set.read(), dtype = np.uint8), (number_of_items))\n",
    "    label_set.close()    \n",
    "    return image_set_data.reshape(image_set_data.shape[0], 1, 28, 28), label_set_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X, y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- all training_data\n",
    "    y -- all training_label\n",
    "    \n",
    "    Return:\n",
    "    X_train -- training_dataset\n",
    "    y_train -- training_labelset\n",
    "    X_cross -- cross_validation_dataset\n",
    "    y_cross -- cross_validation_labelset\n",
    "    \"\"\"\n",
    "    data_count = X.shape[0]\n",
    "    shuffle_order = np.arange(data_count)\n",
    "    np.random.shuffle(shuffle_order)\n",
    "    X_train, y_train = X[shuffle_order][:(4 * data_count) // 5], y[shuffle_order][:(4 * data_count) // 5]\n",
    "    X_cross, y_cross = X[shuffle_order][(4 * data_count) // 5:], y[shuffle_order][(4 * data_count) // 5:]\n",
    "    return X_train, y_train, X_cross, y_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST\n",
    "train_image_name = \"mnist\\\\train-images.idx3-ubyte\"\n",
    "train_label_name = \"mnist\\\\train-labels.idx1-ubyte\"\n",
    "test_image_name = \"mnist\\\\t10k-images.idx3-ubyte\"\n",
    "test_label_name = \"mnist\\\\t10k-labels.idx1-ubyte\"\n",
    "\n",
    "X, y = unpickle_MNIST(train_image_name, train_label_name)\n",
    "X_train, y_train, X_cross, y_cross = shuffle(X, y)\n",
    "X_test, y_test = unpickle_MNIST(test_image_name, test_label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of this picture is 8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d2a0ec44e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing MNIST\n",
    "X = X_train.reshape(48000, 28, 28)\n",
    "i = np.random.choice(range(len(X)))\n",
    "plt.imshow(X[i], interpolation = 'nearest')\n",
    "print(\"Number of this picture is\", y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "\n",
    "def MNIST_train(X, y, lambd = 0, epoch = 7, learning_rate = 0.01, test = False):\n",
    "    global count\n",
    "    np.random.seed(1)\n",
    "    fc_W = dict()\n",
    "    fc_b = dict()\n",
    "    fc_gamma = dict()\n",
    "    fc_beta = dict()\n",
    "    fc_mean = dict()\n",
    "    fc_std = dict()\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    Convolution_layer = [(28 * 28, 9, 10), (28 * 28, 9 * 10, 4)]\n",
    "    FC_layer = [28 * 28 * 4, 128, 10]\n",
    "    order = [len(Convolution_layer), len(FC_layer) - 1]\n",
    "\n",
    "    # Set Convolution Layer\n",
    "    for i, v in enumerate(Convolution_layer):\n",
    "        W, gamma, beta, mean, std = \"CW\" + str(i), \"Cz\" + str(i + 1),\\\n",
    "                                    \"Cz\" + str(i + 1), \"Cz\" + str(i + 1), \"Cz\" + str(i + 1)\n",
    "        n_in, n_f_size, n_f_num = v\n",
    "        fc_W[W] = dict()\n",
    "        fc_W[W][0] = np.random.randn(n_f_size, n_f_num) / np.sqrt(n_f_size / 2)\n",
    "        fc_W[W][1], fc_W[W][2] = np.zeros(fc_W[W][0].shape), np.zeros(fc_W[W][0].shape)\n",
    "        fc_gamma[gamma] = dict()\n",
    "        fc_gamma[gamma][0] = np.random.randn(n_in, n_f_num)\n",
    "        fc_gamma[gamma][1], fc_gamma[gamma][2] = np.zeros(fc_gamma[gamma][0].shape), np.zeros(fc_gamma[gamma][0].shape)\n",
    "        fc_beta[beta] = dict()\n",
    "        fc_beta[beta][0] = np.random.randn(n_in, n_f_num)\n",
    "        fc_beta[beta][1], fc_beta[beta][2] = np.zeros(fc_beta[beta][0].shape), np.zeros(fc_beta[beta][0].shape)\n",
    "        fc_mean[mean] = np.zeros((n_in, n_f_num))\n",
    "        fc_std[std] = np.zeros((n_in, n_f_num))\n",
    "\n",
    "    # Set Fully Connected Area\n",
    "    for i in range(len(FC_layer) - 1):\n",
    "        W, b, gamma, beta, mean, std = \"W\" + str(i), \"b\" + str(i), \"z\" + str(i + 1),\\\n",
    "                                       \"z\" + str(i + 1), \"z\" + str(i + 1), \"z\" + str(i + 1)\n",
    "        n_in, n_out = FC_layer[i], FC_layer[i + 1]\n",
    "        fc_W[W] = dict()\n",
    "        fc_W[W][0] = np.random.randn(n_in, n_out) / np.sqrt(n_in / 2)\n",
    "        fc_W[W][1], fc_W[W][2] = np.zeros(fc_W[W][0].shape), np.zeros(fc_W[W][0].shape)\n",
    "        fc_b[b] = dict()\n",
    "        fc_b[b][0] = np.zeros(n_out)\n",
    "        fc_b[b][1], fc_b[b][2] = np.zeros(fc_b[b][0].shape), np.zeros(fc_b[b][0].shape)\n",
    "        fc_gamma[gamma] = dict()\n",
    "        fc_gamma[gamma][0] = np.random.randn(n_out)\n",
    "        fc_gamma[gamma][1], fc_gamma[gamma][2] = np.zeros(fc_gamma[gamma][0].shape), np.zeros(fc_gamma[gamma][0].shape)\n",
    "        fc_beta[beta] = dict()\n",
    "        fc_beta[beta][0] = np.random.randn(n_out)\n",
    "        fc_beta[beta][1], fc_beta[beta][2] = np.zeros(fc_beta[beta][0].shape), np.zeros(fc_beta[beta][0].shape)\n",
    "        fc_mean[mean] = np.zeros(n_out)\n",
    "        fc_std[std] = np.zeros(n_out)\n",
    "    training_cost = []\n",
    "    # Training Start\n",
    "    for epo in range(epoch):\n",
    "        for i in range(0, X_train.shape[0], 128):\n",
    "            X_mini = X_train[i:i + 128]\n",
    "            y_mini = y_train[i:i + 128]\n",
    "            if test == True:\n",
    "                numerical_check(order, X_mini, y_mini, fc_W, fc_b, fc_gamma, fc_beta, fc_mean, fc_std, lambd)\n",
    "            #forward\n",
    "            cache = forward(order, X_mini, fc_W, fc_b, fc_gamma, fc_beta, fc_mean, fc_std)\n",
    "            if (count - 1) % 100 == 0:\n",
    "                training_cost.append(nn_cost(y_mini, cache['x'][-1], fc_W, lambd))\n",
    "            #backward\n",
    "            backward(order, cache, fc_W, fc_b, fc_gamma, fc_beta, fc_mean, fc_std, y_mini, learning_rate, test, lambd)\n",
    "            if (count - 1) % 100 == 0:\n",
    "                print(count - 1, \"lambda =\", lambd, \", iteration finished, Cost =\", training_cost[-1])\n",
    "            count += 1\n",
    "            if count == 2:\n",
    "                test = False\n",
    "\n",
    "    parameters = [fc_W, fc_b, fc_gamma, fc_beta, fc_mean, fc_std]\n",
    "    count = 1\n",
    "    return training_cost, parameters, order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(order, X_first, W, b, gamma, beta, mean, std):\n",
    "    cache = dict()\n",
    "    cache['x'] = [X_first]\n",
    "    cache['x_col'] = []\n",
    "    cache['z'] = []\n",
    "    cache['z_bar'] = []\n",
    "    cache['z_y'] = []\n",
    "    cache['z_mean'] = []\n",
    "    cache['z_std'] = []\n",
    "    # Convolution\n",
    "    for layer in range(order[0]):\n",
    "        keyW, keyz = \"CW\" + str(layer), \"Cz\" + str(layer + 1)\n",
    "        cache['x_col'].append(im2col(cache['x'][-1]))\n",
    "        # After Convolution\n",
    "        cache['z'].append(conv(cache['x_col'][-1], W[keyW][0]))\n",
    "        # Batch\n",
    "        z_bar, z_y, z_mean, z_std = nn_batch(cache['z'][-1], gamma[keyz][0], beta[keyz][0], mean[keyz], std[keyz])\n",
    "        cache['z_bar'].append(z_bar)\n",
    "        cache['z_y'].append(z_y)\n",
    "        cache['z_mean'].append(z_mean)\n",
    "        cache['z_std'].append(z_std)\n",
    "        if layer != order[0] - 1:\n",
    "            cache['x'].append(conv_leaky_relu(cache['z_y'][-1]))\n",
    "        else:\n",
    "            k = conv_leaky_relu(cache['z_y'][-1])\n",
    "            cache['x'].append(k.reshape((k.shape[0], k.shape[1] * k.shape[2] * k.shape[3])))\n",
    "    # Fully Connected Neural Network\n",
    "    for layer in range(order[1]):\n",
    "        keyW, keyb, keyz = \"W\" + str(layer), \"b\" + str(layer), \"z\" + str(layer + 1)\n",
    "        cache['z'].append(nn_fc(cache['x'][-1], W[keyW][0], b[keyb][0]))\n",
    "        #Batch\n",
    "        z_bar, z_y, z_mean, z_std = nn_batch(cache['z'][-1], gamma[keyz][0], beta[keyz][0], mean[keyz], std[keyz])\n",
    "        cache['z_bar'].append(z_bar)\n",
    "        cache['z_y'].append(z_y)\n",
    "        cache['z_mean'].append(z_mean)\n",
    "        cache['z_std'].append(z_std)\n",
    "        #softmax\n",
    "        if layer == order[1] - 1:\n",
    "            cache['x'].append(nn_softmax(cache['z_y'][-1]))\n",
    "        else:\n",
    "            cache['x'].append(nn_leaky_relu(cache['z_y'][-1]))\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(order, cache, W, b, gamma, beta, mean, std, y, learning_rate, test, lambd):\n",
    "    dy = cache['x'].pop()\n",
    "    #dy /= dy.shape[0]\n",
    "    compare = deque()\n",
    "    for layer in range(order[1] - 1, -1, -1):\n",
    "        keyW, keyb, keyz = \"W\" + str(layer), \"b\" + str(layer), \"z\" + str(layer + 1)\n",
    "        if layer == order[1] - 1:\n",
    "            cache['z_y'].pop()\n",
    "            dy[np.arange(y.shape[0]), y] -= 1\n",
    "            dy /= y.shape[0]\n",
    "        else:\n",
    "            dy[(cache['z_y'].pop()) < 0] *= 0.01\n",
    "        # batch\n",
    "        dy, dgamma, dbeta = nn_grad_batch(cache['z'].pop(), cache['z_bar'].pop(), dy, gamma[keyz][0], \\\n",
    "                                          cache['z_mean'].pop(), cache['z_std'].pop())\n",
    "        if test == True:\n",
    "            compare.append((keyz, \"beta\", dbeta[:3]))\n",
    "            compare.append((keyz, \"gamma\", dgamma[:3]))\n",
    "        update(beta[keyz][0], dbeta, beta[keyz][1], beta[keyz][2], learning_rate)\n",
    "        update(gamma[keyz][0], dgamma, gamma[keyz][1], gamma[keyz][2], learning_rate)\n",
    "        # FC\n",
    "        dW, db, dy = nn_grad(cache['x'].pop(), W[keyW][0], b[keyb][0], dy, lambd)\n",
    "        if test == True:\n",
    "            compare.append((keyb, db[:3]))\n",
    "            compare.append((keyW, dW[:3, :3]))\n",
    "        update(b[keyb][0], db, b[keyb][1], b[keyb][2], learning_rate)\n",
    "        update(W[keyW][0], dW, W[keyW][1], W[keyW][2], learning_rate)\n",
    "    # Convolution\n",
    "    dy = dy.reshape(dy.shape[0], 4, 28, 28)\n",
    "    for layer in range(order[0] - 1, -1, -1):\n",
    "        keyW, keyz = \"CW\" + str(layer), \"Cz\" + str(layer + 1)\n",
    "        # Leaky relu\n",
    "        dy = flatten(dy, (cache['z_y']).pop())\n",
    "        # batch\n",
    "        dy, dgamma, dbeta = nn_grad_batch(cache['z'].pop(), cache['z_bar'].pop(), dy, gamma[keyz][0], \\\n",
    "                                          cache['z_mean'].pop(), cache['z_std'].pop())\n",
    "        if test == True:\n",
    "            compare.append((keyz, \"beta\", dbeta[:3, :3]))\n",
    "            compare.append((keyz, \"gamma\", dgamma[:3, :3]))\n",
    "        update(beta[keyz][0], dbeta, beta[keyz][1], beta[keyz][2], learning_rate)\n",
    "        update(gamma[keyz][0], dgamma, gamma[keyz][1], gamma[keyz][2], learning_rate)\n",
    "        # Conv\n",
    "        dW, dy = nn_conv_grad(cache['x_col'].pop(), W[keyW][0], dy, lambd)\n",
    "        if test == True:\n",
    "            compare.append((keyW, dW[:3, :3]))\n",
    "        update(W[keyW][0], dW, W[keyW][1], W[keyW][2], learning_rate)\n",
    "        dy = col2im(dy, cache['x'].pop())\n",
    "    if test == True:\n",
    "        print(\"Derivation\")\n",
    "        while len(compare) > 0:\n",
    "            a = compare.pop()\n",
    "            print(a[:-1])\n",
    "            print(a[-1])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(X):\n",
    "    temp = np.pad(X, ((0, 0), (0, 0), (1, 1), (1, 1)), 'constant', constant_values = ((0, 0), (0, 0), (0, 0), (0, 0)))\n",
    "    X_col = np.zeros((X.shape[0], X.shape[2] * X.shape[3], 9 * X.shape[1]))\n",
    "    for n in range(X.shape[0]):\n",
    "        count = 0\n",
    "        for i in range(X.shape[2]):\n",
    "            for j in range(X.shape[3]):\n",
    "                X_col[n, count] = temp[n, :, i:i + 3, j:j + 3].reshape(9 * X.shape[1])\n",
    "                count += 1\n",
    "    return X_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(X, W):\n",
    "    # X = N, 28 * 28, 9 * 8, W = 9 * 8, 4\n",
    "    # y = N, 28 * 28, 4\n",
    "    y = np.matmul(X, W)#.transpose(0, 2, 1).reshape((X.shape[0], W.shape[1], 28, 28)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_batch(z, z_gamma, z_beta, z_mean, z_std, epsilon = 0.000001):\n",
    "    mean = z.mean(axis = 0)\n",
    "    std = (z.var(axis = 0) + epsilon) ** (1 / 2)\n",
    "    z_mean *= 0.99\n",
    "    z_mean += 0.01 * mean\n",
    "    z_std *= 0.99\n",
    "    z_std += 0.01 * std\n",
    "    z_x_bar = (z - mean) / std\n",
    "    z_y = z_gamma * z_x_bar + z_beta\n",
    "    return z_x_bar, z_y, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_leaky_relu(X):\n",
    "    y = X.transpose(0, 2, 1).reshape(X.shape[0], X.shape[2], 28, 28)\n",
    "    y[y < 0] *= 0.01\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_fc(X, W, b):\n",
    "    return np.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_leaky_relu(X):\n",
    "    y = X.copy()\n",
    "    y[y < 0] *= 0.01\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_softmax(x):\n",
    "    y = np.exp(x)\n",
    "    y /= y.sum(axis = 1, keepdims = True)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cost(y, y_hat, W, lambd):\n",
    "    cost = np.sum(-np.log(y_hat[np.arange(y_hat.shape[0]), y])) / y.shape[0]\n",
    "    for key in W:\n",
    "        cost += np.sum(W[key][0] ** 2) * (lambd / (2 * y.shape[0]))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_grad_batch(x, x_bar, dy, gamma, mean, std):\n",
    "    m = x.shape[0]\n",
    "    dx_bar = dy * gamma\n",
    "    dx = (1 / (m * std)) * (m * dx_bar - np.sum(dx_bar, axis=0) - x_bar * np.sum(dx_bar * x_bar, axis=0))\n",
    "    dgamma = np.sum(dy * x_bar, axis = 0)\n",
    "    dbeta = np.sum(dy, axis = 0)\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_grad(x, W, b, dy, lambd):\n",
    "    dW = np.matmul(x.T, dy) + (lambd / x.shape[0]) * W\n",
    "    db = np.sum(dy, axis = 0)\n",
    "    dX = np.matmul(dy, W.T)\n",
    "    return dW, db, dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(dy, y):\n",
    "    dy = dy.reshape(y.shape[0], y.shape[2], y.shape[1]).transpose(0, 2, 1)\n",
    "    dy[y < 0] *= 0.01\n",
    "    return dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im(X_col, X):\n",
    "    X_im = np.zeros((X.shape[0], X.shape[1], X.shape[2] + 2, X.shape[3] + 2))\n",
    "    for n in range(X_col.shape[0]):\n",
    "        a, b = 0, 0\n",
    "        for i in range(X_col.shape[1]):\n",
    "            X_im[n, :, a:a + 3, b:b + 3] += X_col[n, i].reshape(X.shape[1], 3, 3)\n",
    "            b += 1\n",
    "            if b + 3 > X_im.shape[3]:\n",
    "                a += 1\n",
    "                b = 0\n",
    "    return X_im[:, :, 1:-1, 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_conv_grad(x, W, dy, lambd):\n",
    "    dW = np.sum(np.matmul(x.transpose(0, 2, 1), dy), axis = 0) + (lambd / x.shape[0]) * W\n",
    "    dX = np.matmul(dy, W.T)\n",
    "    return dW, dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(W, dW, Wm, Wv, alpha, beta1 = 0.9, beta2 = 0.999, epsilon = (10 ** (-8))):\n",
    "    global count\n",
    "    alpha *= (1 / 2) ** (count / 100)\n",
    "    Wm *= beta1\n",
    "    Wm += (1 - beta1) * dW\n",
    "    Wv *= beta2\n",
    "    Wv += (1 - beta2) * (dW ** 2)\n",
    "    W -= (alpha * Wm / (1 - (beta1 ** count))) / np.sqrt((Wv / (1 - (beta2 ** count))) + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_check(order, X_first, y, W, b, gamma, beta, mean, std, lambd, h = 0.00001):\n",
    "    print(\"Numerical\")\n",
    "    #Convolution\n",
    "    for layer in range(order[0]):\n",
    "        keyW, keyz = \"CW\" + str(layer), \"Cz\" + str(layer + 1)\n",
    "        W_test = np.zeros((3, 3))\n",
    "        gamma_test = np.zeros((3, 3))\n",
    "        beta_test = np.zeros((3, 3))\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                W[keyW][0][i][j] += h\n",
    "                cost1 = numerical_forward(order, X_first, y, W, b, gamma, beta, lambd)\n",
    "                W[keyW][0][i][j] -= 2 * h\n",
    "                cost2 = numerical_forward(order, X_first, y, W, b, gamma, beta, lambd)\n",
    "                W_test[i][j] = (cost1 - cost2) / (2 * h)\n",
    "                W[keyW][0][i][j] += h\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                gamma[keyz][0][i][j] += h\n",
    "                cost1 = numerical_forward(order, X_first, y, W, b, gamma, beta, lambd)\n",
    "                gamma[keyz][0][i][j] -= 2 * h\n",
    "                cost2 = numerical_forward(order, X_first, y, W, b, gamma, beta, lambd)\n",
    "                gamma_test[i][j] = (cost1 - cost2) / (2 * h)\n",
    "                gamma[keyz][0][i][j] += h\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                beta[keyz][0][i][j] += h\n",
    "                cost1 = numerical_forward(order, X_first, y, W, b, gamma, beta, lambd)\n",
    "                beta[keyz][0][i][j] -= 2 * h\n",
    "                cost2 = numerical_forward(order, X_first, y, W, b, gamma, beta, lambd)\n",
    "                beta_test[i][j] = (cost1 - cost2) / (2 * h)\n",
    "                beta[keyz][0][i][j] += h\n",
    "        print((keyW))\n",
    "        print(W_test)\n",
    "        print()\n",
    "        print((keyz, \"gamma\"))\n",
    "        print(gamma_test)\n",
    "        print()\n",
    "        print((keyz, \"beta\"))\n",
    "        print(beta_test)\n",
    "    #Batch\n",
    "    for layer in range(order[1]):\n",
    "        keyW, keyb, keyz = \"W\" + str(layer), \"b\" + str(layer), \"z\" + str(layer + 1)\n",
    "        W_test = np.zeros((3, 3))\n",
    "        b_test = np.zeros(3)\n",
    "        gamma_test = np.zeros(3)\n",
    "        beta_test = np.zeros(3)\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                W[keyW][0][i][j] += h\n",
    "                cost1 = numerical_forward(order, X_first, y, W, b, gamma, beta, lambd)\n",
    "                W[keyW][0][i][j] -= 2 * h\n",
    "                cost2 = numerical_forward(order, X_first, y, W, b, gamma, beta, lambd)\n",
    "                W_test[i][j] = (cost1 - cost2) / (2 * h)\n",
    "                W[keyW][0][i][j] += h\n",
    "        for i in range(3):\n",
    "            b[keyb][0][i] += h\n",
    "            cost1 = numerical_forward(order, X_first, y, W, b, gamma, beta, lambd)\n",
    "            b[keyb][0][i] -= 2 * h\n",
    "            cost2 = numerical_forward(order, X_first, y, W, b, gamma, beta, lambd)\n",
    "            b_test[i] = (cost1 - cost2) / (2 * h)\n",
    "            b[keyb][0][i] += h\n",
    "        for i in range(3):\n",
    "            gamma[keyz][0][i] += h\n",
    "            cost1 = numerical_forward(order, X_first, y, W, b, gamma, beta, lambd)\n",
    "            gamma[keyz][0][i] -= 2 * h\n",
    "            cost2 = numerical_forward(order, X_first, y, W, b, gamma, beta, lambd)\n",
    "            gamma_test[i] = (cost1 - cost2) / (2 * h)\n",
    "            gamma[keyz][0][i] += h\n",
    "        for i in range(3):\n",
    "            beta[keyz][0][i] += h\n",
    "            cost1 = numerical_forward(order, X_first, y, W, b, gamma, beta, lambd)\n",
    "            beta[keyz][0][i] -= 2 * h\n",
    "            cost2 = numerical_forward(order, X_first, y, W, b, gamma, beta, lambd)\n",
    "            beta_test[i] = (cost1 - cost2) / (2 * h)\n",
    "            beta[keyz][0][i] += h\n",
    "        print((keyW))\n",
    "        print(W_test)\n",
    "        print()\n",
    "        print((keyb))\n",
    "        print(b_test)\n",
    "        print()\n",
    "        print((keyz, \"gamma\"))\n",
    "        print(gamma_test)\n",
    "        print()\n",
    "        print((keyz, \"beta\"))\n",
    "        print(beta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_forward(order, X_first, y, W, b, gamma, beta, lambd):\n",
    "    #Convolution\n",
    "    y_hat = X_first\n",
    "    for layer in range(order[0]):\n",
    "        keyW, keyz = \"CW\" + str(layer), \"Cz\" + str(layer + 1)\n",
    "        y_hat = im2col(y_hat)\n",
    "        # After Convolution\n",
    "        y_hat = conv(y_hat, W[keyW][0])\n",
    "        # Batch\n",
    "        y_hat = numerical_batch(y_hat, gamma[keyz][0], beta[keyz][0])\n",
    "        if layer != order[0] - 1:\n",
    "            y_hat = conv_leaky_relu(y_hat)\n",
    "        else:\n",
    "            k = conv_leaky_relu(y_hat)\n",
    "            y_hat = k.reshape(k.shape[0], k.shape[1] * k.shape[2] * k.shape[3])\n",
    "    #Batch\n",
    "    for layer in range(order[1]):\n",
    "        keyW, keyb, keyz = \"W\" + str(layer), \"b\" + str(layer), \"z\" + str(layer + 1)\n",
    "        y_hat = nn_fc(y_hat, W[keyW][0], b[keyb][0])\n",
    "        #Batch\n",
    "        y_hat = numerical_batch(y_hat, gamma[keyz][0], beta[keyz][0])\n",
    "        #softmax\n",
    "        if layer == order[1] - 1:\n",
    "            y_hat = nn_softmax(y_hat)\n",
    "        else:\n",
    "            y_hat = nn_leaky_relu(y_hat)\n",
    "    cost = nn_cost(y, y_hat, W, lambd)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_batch(z, z_gamma, z_beta, epsilon = 0.000001):\n",
    "    mean = z.mean(axis = 0)\n",
    "    std = (z.var(axis = 0) + epsilon) ** (1 / 2)\n",
    "    z_x_bar = (z - mean) / std\n",
    "    z_y = z_gamma * z_x_bar + z_beta\n",
    "    return z_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters, order):\n",
    "    y_hat = test_forward(X, y, parameters, order)\n",
    "    y_hat = np.argmax(y_hat, axis = 1)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, y, parameters, order):\n",
    "    y_hat = test_forward(X, y, parameters, order)\n",
    "    y_hat = np.argmax(y_hat, axis = 1)\n",
    "    return np.sum(y == y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward(X, y, parameters, order):\n",
    "    y_hat = X\n",
    "    W, b, gamma, beta, mean, std = parameters\n",
    "    for layer in range(order[0]):\n",
    "        keyW, keyz = \"CW\" + str(layer), \"Cz\" + str(layer + 1)\n",
    "        y_hat = im2col(y_hat)\n",
    "        y_hat = conv(y_hat, W[keyW][0])\n",
    "        y_hat = test_batch(y_hat, mean[keyz], std[keyz], gamma[keyz][0], beta[keyz][0])\n",
    "        if layer != order[0] - 1:\n",
    "            y_hat = conv_leaky_relu(y_hat)\n",
    "        else:\n",
    "            k = conv_leaky_relu(y_hat)\n",
    "            y_hat = k.reshape(k.shape[0], k.shape[1] * k.shape[2] * k.shape[3])\n",
    "\n",
    "    for layer in range(order[1]):\n",
    "        keyW, keyb, keyz = \"W\" + str(layer), \"b\" + str(layer), \"z\" + str(layer + 1)\n",
    "        y_hat = nn_fc(y_hat, W[keyW][0], b[keyb][0])\n",
    "        #Batch\n",
    "        y_hat = test_batch(y_hat, mean[keyz], std[keyz], gamma[keyz][0], beta[keyz][0])\n",
    "        #softmax\n",
    "        if layer != order[1] - 1:\n",
    "            y_hat = nn_leaky_relu(y_hat)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(z, mean, std, z_gamma, z_beta):\n",
    "    z_x_bar = (z - mean) / std\n",
    "    z_y = z_gamma * z_x_bar + z_beta\n",
    "    return z_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 lambda = 0 , iteration finished, Cost = 3.07143362129\n",
      "100 lambda = 0 , iteration finished, Cost = 0.440736557684\n",
      "200 lambda = 0 , iteration finished, Cost = 0.310685860235\n",
      "300 lambda = 0 , iteration finished, Cost = 0.188330600177\n",
      "400 lambda = 0 , iteration finished, Cost = 0.215203676572\n",
      "500 lambda = 0 , iteration finished, Cost = 0.151857726934\n",
      "600 lambda = 0 , iteration finished, Cost = 0.202990221467\n",
      "700 lambda = 0 , iteration finished, Cost = 0.171946274107\n",
      "800 lambda = 0 , iteration finished, Cost = 0.241236623658\n",
      "900 lambda = 0 , iteration finished, Cost = 0.169669222248\n",
      "1000 lambda = 0 , iteration finished, Cost = 0.17798442258\n",
      "1100 lambda = 0 , iteration finished, Cost = 0.17059395453\n",
      "1200 lambda = 0 , iteration finished, Cost = 0.153190464082\n",
      "1300 lambda = 0 , iteration finished, Cost = 0.135935448401\n",
      "1400 lambda = 0 , iteration finished, Cost = 0.185079251189\n",
      "1500 lambda = 0 , iteration finished, Cost = 0.179048854119\n",
      "1600 lambda = 0 , iteration finished, Cost = 0.1438022563\n",
      "1700 lambda = 0 , iteration finished, Cost = 0.159703055057\n",
      "1800 lambda = 0 , iteration finished, Cost = 0.134431005772\n",
      "1900 lambda = 0 , iteration finished, Cost = 0.177027833171\n",
      "2000 lambda = 0 , iteration finished, Cost = 0.133699740264\n",
      "2100 lambda = 0 , iteration finished, Cost = 0.193286737189\n",
      "2200 lambda = 0 , iteration finished, Cost = 0.167318601892\n",
      "2300 lambda = 0 , iteration finished, Cost = 0.238696941117\n",
      "2400 lambda = 0 , iteration finished, Cost = 0.168405430191\n",
      "2500 lambda = 0 , iteration finished, Cost = 0.177117101528\n",
      "2600 lambda = 0 , iteration finished, Cost = 0.170345001135\n"
     ]
    }
   ],
   "source": [
    "L2regul = [0]\n",
    "\n",
    "accuracy_train = []\n",
    "accuracy_cross = []\n",
    "lambd_cost = []\n",
    "lambd_parameters = []\n",
    "\n",
    "for lamb in L2regul:\n",
    "    cost, parameters, order = MNIST_train(X_train, y_train, lambd = lamb)\n",
    "    lambd_parameters.append(parameters)\n",
    "    lambd_cost.append(cost)\n",
    "    acc = 0\n",
    "    for i in range(0, X_train.shape[0], 128):\n",
    "        acc += accuracy(X_train[i:i + 128], y_train[i:i + 128], parameters, order)\n",
    "    accuracy_train.append(acc / X_train.shape[0] * 100)\n",
    "    acc = 0\n",
    "    for i in range(0, X_cross.shape[0], 128):\n",
    "        acc += accuracy(X_cross[i:i + 128], y_cross[i:i + 128], parameters, order)\n",
    "    accuracy_cross.append(acc / X_cross.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96.0625] [95.183333333333337]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x1d2a1f07d68>],\n",
       " <a list of 1 Text xticklabel objects>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0OS1OT/AZH9C/b+RhkAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d2a2185f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(accuracy_train, accuracy_cross)\n",
    "\n",
    "plt.plot(range(len(L2regul[:2])), accuracy_train)\n",
    "plt.plot(range(len(L2regul[:2])), accuracy_cross)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.title(\"ERROR\")\n",
    "plt.xticks(range(len(L2regul[:2])), L2regul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 95.62 %\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(0, X_test.shape[0], 512):\n",
    "    acc += accuracy(X_test[i:i + 512], y_test[i:i + 512], lambd_parameters[0], order)\n",
    "acc /= X_test.shape[0]\n",
    "\n",
    "print(\"Test accuracy is\", acc * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real class is 7\n",
      "predict = 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqf8HjA7F/brvy+YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d2a2f41390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.choice(range(len(X_test)))\n",
    "plt.imshow(X_test[i].reshape((28, 28)), interpolation='nearest')\n",
    "print(\"real class is\", y_test[i])\n",
    "print(\"predict =\", predict(X_test[i:i + 1], y_test[i:i + 1], lambd_parameters[0], order)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(lambd_cost[0])), lambd_cost[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
